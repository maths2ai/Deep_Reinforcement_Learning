{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "\n",
    "## We are gonna develop PPO algorithm which will be the base for other algorithms like:\n",
    "## 1) Playing Montezuma Revange from a single demostration\n",
    "## 2) Go-Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Section\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "from Parallel_env import parallelEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = 'PongDeterministic-v4'\n",
    "\n",
    "num_episodes = 2000\n",
    "beta_decay = 0.995\n",
    "beta = 0.01\n",
    "discount_rate = 0.99\n",
    "tmax = 400\n",
    "epsilon = 0.1 # For clipping\n",
    "epsilon_decay = 0.999\n",
    "recycling_traj = 4\n",
    "\n",
    "mean_rewards = []\n",
    "\n",
    "RIGHT = 4\n",
    "LEFT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_single(image, bkg_color = np.array([144, 72, 17])):\n",
    "    # NOTICE: The image 'image' we are receving is 260*160. We want to remove part of the pixels\n",
    "    # from the first axis and rescale the image in a form 80*80. We put everything in grayscale.\n",
    "    img = np.mean(image[34:-16:2,::2] - bkg_color, axis = 1)/255.\n",
    "    return img\n",
    "\n",
    "def preprocess_batch(images, bkg_color = np.array([144, 72, 17])):\n",
    "    array_images = np.asarray(images)\n",
    "    \n",
    "    # Next if is used just if we do not parallelize\n",
    "    if len(array_images.shape) < 5:\n",
    "        array_images = np.expand_dims(array_images, axis = 1)\n",
    "    \n",
    "    # First dim of array_images represents the 2 subsequent frames. The second the number of envs.\n",
    "    array_images_prepro = np.mean(array_images[:,:, 34:-16:2, ::2] - bkg_color, axis = -1)/255.0\n",
    "    batch_input = torch.from_numpy(np.swapaxes(array_images_prepro, 0, 1)).float().to(device)\n",
    "    \n",
    "    return batch_input  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Trajectory function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectT(envs, policy, tmax = 320, nrandom = 5):\n",
    "    \n",
    "    n = len(envs.ps)\n",
    "    \n",
    "    state_list = []\n",
    "    reward_list = []\n",
    "    prob_list = []\n",
    "    action_list = []\n",
    "    \n",
    "    envs.reset()\n",
    "    \n",
    "    exp1, exp2, exp3, exp4 = envs.step([1]*n)\n",
    "    \n",
    "    # Performing some random steps\n",
    "    \n",
    "    for _ in range(nrandom):\n",
    "        frame1, reward1, is_done1, info1 = envs.step(np.random.choice([RIGHT, LEFT],n))\n",
    "        frame2, reward2, is_done2, info2 = envs.step([0]*n)\n",
    "   \n",
    "    for t in range(tmax):\n",
    "        # Stacking two frames on top of each other, rescaling and giving the tensor the shape\n",
    "        # n*2*80*80\n",
    "        batch_input = preprocess_batch([frame1, frame2]) \n",
    "        \n",
    "        ## HERE WHY SQUEEZE? TRY TO UNDERSTAND WHY\n",
    "        probs = policy(batch_input).squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        actions = np.where(np.random.rand(n) < probs, RIGHT, LEFT)\n",
    "        probs = np.where(actions == RIGHT, probs, 1.0 - probs)\n",
    "        \n",
    "        frame1, reward1, is_done, info1 = envs.step(actions)\n",
    "        frame2, reward2, is_done, info2 = envs.step([0]*n)\n",
    "        \n",
    "        reward = reward1 + reward2\n",
    "        \n",
    "        state_list.append(batch_input)\n",
    "        reward_list.append(reward)\n",
    "        prob_list.append(probs)\n",
    "        action_list.append(actions)\n",
    "        \n",
    "        if is_done.any():\n",
    "            break\n",
    "    \n",
    "    # HERE IN THE MAIN I JUST USE PROBS AND REWARDS I THINK THEY ARE THE ONLY NEEDED IN MY LOSS FUNCTION\n",
    "    return prob_list, state_list, action_list, reward_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Play function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy, preprocess=None, nrand=5):\n",
    "    #env.reset()\n",
    "\n",
    "    # star game\n",
    "    env.step(1)\n",
    "    \n",
    "    # perform nrand random steps in the beginning\n",
    "    for _ in range(nrand):\n",
    "        frame1, reward1, is_done1, info1 = env.step(np.random.choice([RIGHT,LEFT]))\n",
    "        frame2, reward2, is_done2, info2 = env.step(0)\n",
    "    \n",
    "    while True:\n",
    "       \n",
    "        frame_input = preprocess_batch([frame1, frame2])\n",
    "        prob = policy(frame_input)\n",
    "        \n",
    "        # RIGHT = 4, LEFT = 5\n",
    "        action = RIGHT if rand.random() < prob else LEFT\n",
    "        frame1, reward1 , is_done1, info1 = env.step(action)\n",
    "        \n",
    "        if is_done1 == True:\n",
    "            #env.reset()\n",
    "            break\n",
    "        frame2, reward2 , is_done2, info2 = env.step(0)\n",
    "    \n",
    "        if is_done2 == True:\n",
    "            #env.reset()\n",
    "            break\n",
    "    \n",
    "    #env.close()\n",
    "    \n",
    "    #animate_frames(anim_frames)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate(policy, old_probs, states, actions, rewards, discount, epsilon = 0.1, beta = 0.1):\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rew_disc = np.asarray(rewards) * discount[:, np.newaxis]\n",
    "    rew_future = rew_disc[::-1].cumsum(axis = 0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rew_future, axis = 1)\n",
    "    std = np.std(rew_future, axis = 1) + 1.0e-10\n",
    "    \n",
    "    rewards_normalized = (rew_future - mean[:, np.newaxis])/(std[:, np.newaxis])\n",
    "    \n",
    "    actions = torch.tensor(actions, dtype = torch.int8).to(device)\n",
    "    old_probs = torch.tensor(old_probs, dtype = torch.float).to(device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype = torch.float).to(device)\n",
    "    \n",
    "    states = torch.stack(states).to(device)\n",
    "    policy_input = states.view(-1, *states.shape[-3:]) # Reshaping. First axis is batch\n",
    "    probs_new_policy = policy(policy_input).view(states.shape[:-3])\n",
    "    probs_new_policy = torch.where(actions == RIGHT, probs_new_policy, 1.0 - probs_new_policy)\n",
    "    \n",
    "    ratio = probs_new_policy/old_probs\n",
    "    \n",
    "    clip = torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon) \n",
    "    clipped_surrogate = torch.min(ratio * rewards, clip * rewards)\n",
    "    \n",
    "    # MIXED ENTROPY\n",
    "    \n",
    "    entropy = -(probs_new_policy*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-probs_new_policy)*torch.log(1.0-old_probs+1.e-10))\n",
    "    \n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        ## THIS HAS TO BE MODIFIED USING SEQUENTIAL\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size = 6, stride = 2, bias = False) \n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size = 6, stride = 4)\n",
    "        self.size = 9*9*16\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.size, 256)\n",
    "        self.linear2 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, state):\n",
    "        state = F.relu(self.conv1(state))\n",
    "        state = F.relu(self.conv2(state))\n",
    "        state = state.view(-1, self.size)\n",
    "        state = F.relu(self.linear1(state))\n",
    "        return self.sig(self.linear2(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instatiating objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-16:\n",
      "Process Process-10:\n",
      "Process Process-9:\n",
      "Process Process-12:\n",
      "Process Process-13:\n",
      "Process Process-15:\n",
      "Process Process-11:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-14:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/m2ai/Desktop/AI/Deep Reinforcement Learning/Codes/Policy Methods/Reinforce/Reinforce by Udacity/Reinforce/Parallel_env_my_version.py\", line 24, in worker\n",
      "    cmd, data = child.recv()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/m2ai/Desktop/AI/Deep Reinforcement Learning/Codes/Policy Methods/Reinforce/Reinforce by Udacity/Reinforce/Parallel_env_my_version.py\", line 24, in worker\n",
      "    cmd, data = child.recv()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/m2ai/Desktop/AI/Deep Reinforcement Learning/Codes/Policy Methods/Reinforce/Reinforce by Udacity/Reinforce/Parallel_env_my_version.py\", line 24, in worker\n",
      "    cmd, data = child.recv()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/m2ai/Desktop/AI/Deep Reinforcement Learning/Codes/Policy Methods/Reinforce/Reinforce by Udacity/Reinforce/Parallel_env_my_version.py\", line 24, in worker\n",
      "    cmd, data = child.recv()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/m2ai/Desktop/AI/Deep Reinforcement Learning/Codes/Policy Methods/Reinforce/Reinforce by Udacity/Reinforce/Parallel_env_my_version.py\", line 24, in worker\n",
      "    cmd, data = child.recv()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/m2ai/Desktop/AI/Deep Reinforcement Learning/Codes/Policy Methods/Reinforce/Reinforce by Udacity/Reinforce/Parallel_env_my_version.py\", line 24, in worker\n",
      "    cmd, data = child.recv()\n",
      "  File \"/home/m2ai/Desktop/AI/Deep Reinforcement Learning/Codes/Policy Methods/Reinforce/Reinforce by Udacity/Reinforce/Parallel_env_my_version.py\", line 24, in worker\n",
      "    cmd, data = child.recv()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/m2ai/Desktop/AI/Deep Reinforcement Learning/Codes/Policy Methods/Reinforce/Reinforce by Udacity/Reinforce/Parallel_env_my_version.py\", line 24, in worker\n",
      "    cmd, data = child.recv()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "agent = Policy().to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr = 1e-4)\n",
    "envs = parallelEnv(environment, n = 8, seed = 12345)\n",
    "# Next is just for video\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = gym.wrappers.Monitor(env, \"./vid\" , video_callable=lambda episode_id: (episode_id+1)%10==0, force = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10 , mean reward -18.75\n",
      "Episode: 20 , mean reward -16.88\n",
      "Episode: 30 , mean reward -18.38\n",
      "Episode: 40 , mean reward -17.62\n",
      "Episode: 50 , mean reward -16.88\n",
      "Episode: 60 , mean reward -17.12\n",
      "Episode: 70 , mean reward -17.00\n",
      "Episode: 80 , mean reward -16.88\n",
      "Episode: 90 , mean reward -16.88\n",
      "Episode: 100 , mean reward -15.62\n",
      "Episode: 110 , mean reward -15.38\n",
      "Episode: 120 , mean reward -13.88\n",
      "Episode: 130 , mean reward -13.38\n",
      "Episode: 140 , mean reward -14.12\n",
      "Episode: 150 , mean reward -14.75\n",
      "Episode: 160 , mean reward -11.88\n",
      "Episode: 170 , mean reward -11.00\n",
      "Episode: 180 , mean reward -12.25\n",
      "Episode: 190 , mean reward -10.25\n",
      "Episode: 200 , mean reward -12.00\n",
      "Episode: 210 , mean reward -10.25\n",
      "Episode: 220 , mean reward -11.00\n",
      "Episode: 230 , mean reward -10.12\n",
      "Episode: 240 , mean reward -10.75\n",
      "Episode: 250 , mean reward -7.62\n",
      "Episode: 260 , mean reward -6.00\n",
      "Episode: 270 , mean reward -6.88\n",
      "Episode: 280 , mean reward -7.25\n",
      "Episode: 290 , mean reward -6.88\n",
      "Episode: 300 , mean reward -3.88\n",
      "Episode: 310 , mean reward -3.12\n",
      "Episode: 320 , mean reward -3.12\n",
      "Episode: 330 , mean reward -3.25\n",
      "Episode: 340 , mean reward -3.38\n",
      "Episode: 350 , mean reward -1.88\n",
      "Episode: 360 , mean reward -0.50\n",
      "Episode: 370 , mean reward -1.38\n",
      "Episode: 380 , mean reward -1.88\n",
      "Episode: 390 , mean reward 0.12\n",
      "Episode: 400 , mean reward -0.50\n",
      "Episode: 410 , mean reward 0.75\n",
      "Episode: 420 , mean reward 0.62\n",
      "Episode: 430 , mean reward 2.00\n",
      "Episode: 440 , mean reward 1.00\n",
      "Episode: 450 , mean reward 2.00\n",
      "Episode: 460 , mean reward 2.50\n",
      "Episode: 470 , mean reward 3.38\n",
      "Episode: 480 , mean reward 2.38\n",
      "Episode: 490 , mean reward 3.88\n",
      "Episode: 500 , mean reward 4.25\n",
      "Episode: 510 , mean reward 5.00\n",
      "Episode: 520 , mean reward 5.50\n",
      "Episode: 530 , mean reward 4.62\n",
      "Episode: 540 , mean reward 4.25\n",
      "Episode: 550 , mean reward 5.75\n",
      "Episode: 560 , mean reward 4.12\n",
      "Episode: 570 , mean reward 4.88\n",
      "Episode: 580 , mean reward 5.00\n",
      "Episode: 590 , mean reward 5.12\n",
      "Episode: 600 , mean reward 5.50\n",
      "Episode: 610 , mean reward 6.00\n",
      "Episode: 620 , mean reward 6.00\n",
      "Episode: 630 , mean reward 6.00\n",
      "Episode: 640 , mean reward 5.50\n",
      "Episode: 650 , mean reward 5.25\n",
      "Episode: 660 , mean reward 6.00\n",
      "Episode: 670 , mean reward 5.88\n",
      "Episode: 680 , mean reward 6.00\n",
      "Episode: 690 , mean reward 5.50\n",
      "Episode: 700 , mean reward 5.75\n",
      "Episode: 710 , mean reward 5.75\n",
      "Episode: 720 , mean reward 5.75\n",
      "Episode: 730 , mean reward 6.00\n",
      "Episode: 740 , mean reward 5.88\n",
      "Episode: 750 , mean reward 6.00\n",
      "Episode: 760 , mean reward 5.75\n",
      "Episode: 770 , mean reward 6.00\n",
      "Episode: 780 , mean reward 5.75\n",
      "Episode: 790 , mean reward 5.75\n",
      "Episode: 800 , mean reward 6.00\n",
      "Episode: 810 , mean reward 6.00\n",
      "Episode: 820 , mean reward 5.62\n",
      "Episode: 830 , mean reward 6.00\n",
      "Episode: 840 , mean reward 5.50\n",
      "Episode: 850 , mean reward 6.00\n",
      "Episode: 860 , mean reward 5.75\n",
      "Episode: 870 , mean reward 6.00\n",
      "Episode: 880 , mean reward 5.75\n",
      "Episode: 890 , mean reward 6.00\n",
      "Episode: 900 , mean reward 5.75\n",
      "Episode: 910 , mean reward 5.38\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-cd96a291ebde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mold_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollectT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e4140aa3f3bb>\u001b[0m in \u001b[0;36mcollectT\u001b[0;34m(envs, policy, tmax, nrandom)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Stacking two frames on top of each other, rescaling and giving the tensor the shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# n*2*80*80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mbatch_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m## HERE WHY SQUEEZE? TRY TO UNDERSTAND WHY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1ee4f98355b8>\u001b[0m in \u001b[0;36mpreprocess_batch\u001b[0;34m(images, bkg_color)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# First dim of array_images represents the 2 subsequent frames. The second the number of envs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0marray_images_prepro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbkg_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mbatch_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_images_prepro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3254\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3256\u001b[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[1;32m   3257\u001b[0m                           out=out, **kwargs)\n\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "beta = 0.1\n",
    "for ep in range(num_episodes):\n",
    "    old_probs, states, actions, rewards = collectT(envs, agent, tmax = tmax)\n",
    "    \n",
    "    total_rewards = np.sum(rewards, axis = 0)\n",
    "    \n",
    "    for _ in range(recycling_traj):\n",
    "        L = - surrogate(agent, old_probs, states, actions, rewards, discount_rate, epsilon, beta)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epsilon *= epsilon_decay\n",
    "    beta *= beta_decay\n",
    "    \n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    if (ep + 1)% 10 == 0:\n",
    "        print('Episode: %.2d , mean reward %.2f'%(ep + 1, np.mean(total_rewards) ))\n",
    "    \n",
    "    env.reset()\n",
    "    play(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
