{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we need to add some comments on the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import section\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "\n",
    "ENV = 'CartPole-v0'\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "HIDDEN_SIZE = 128\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the Net/Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_space, hidden_space, output_space):\n",
    "        super(Net, self).__init__()\n",
    "        self.obs_space = obs_space\n",
    "        self.hidden_space = hidden_space\n",
    "        self.output_space = output_space\n",
    "        \n",
    "        self.Linear_1 = nn.Linear(self.obs_space, self.hidden_space)\n",
    "        self.Linear_2 = nn.Linear(self.hidden_space, self.output_space)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        output_1 = F.relu(self.Linear_1(state))\n",
    "        output_final = self.Linear_2(output_1)\n",
    "        return output_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation of the environment, the Agent and some auxiliary data-structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV)\n",
    "env = gym.wrappers.Monitor(env, \"./vid\", video_callable=lambda episode_id: True,force=True)\n",
    "net = Net(env.observation_space.shape[0], HIDDEN_SIZE, env.action_space.n)\n",
    "\n",
    "Episode = namedtuple('Episode', field_names = ['reward', 'steps'])\n",
    "Episode_Steps = namedtuple('Episode_steps', field_names = ['observation', 'action'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the optimizer, Criterion etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions needed for the collection of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch_Selection(environment, agent, batch_size):\n",
    "    episode_reward = 0\n",
    "    state = environment.reset()\n",
    "    episode = []\n",
    "    list_of_episodes_and_rewards = []\n",
    "    sm = nn.Softmax(dim = 1)\n",
    "    \n",
    "    while True:\n",
    "        state = torch.FloatTensor([state])       \n",
    "        actions_logits = agent(state)\n",
    "        actions_probs = sm(actions_logits).data.cpu().numpy()[0] # [0] is done to eliminate the batch dimension\n",
    "        final_action = np.random.choice(len(actions_probs), p = actions_probs)\n",
    "        new_state, reward, is_done, _ = environment.step(final_action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = state.squeeze(0).data.cpu().numpy()\n",
    "        \n",
    "        one_step_in_environment = Episode_Steps(observation = state, action = final_action)\n",
    "        episode.append(one_step_in_environment)\n",
    "        \n",
    "        if is_done == True:\n",
    "            completed_episode = Episode(reward = episode_reward, steps = episode)\n",
    "            list_of_episodes_and_rewards.append(completed_episode)\n",
    "            \n",
    "            if len(list_of_episodes_and_rewards) == batch_size:\n",
    "                yield list_of_episodes_and_rewards\n",
    "                list_of_episodes_and_rewards = []\n",
    "                \n",
    "            state = environment.reset()\n",
    "            episode_reward = 0\n",
    "            episode = []\n",
    "            \n",
    "        state = new_state\n",
    "    \n",
    "def Top_Percentile(episodes, percentile):\n",
    "    rewards = list(map(lambda x : x.reward, batch))\n",
    "    reward_treshold = np.percentile(rewards, percentile)\n",
    "    reward_mean = np.mean(rewards)  \n",
    "    obs = []\n",
    "    action = []\n",
    "    \n",
    "    for trajectory in batch:\n",
    "        if trajectory.reward >= reward_treshold:\n",
    "            obs.extend(map(lambda step: step.observation, trajectory.steps))\n",
    "            action.extend(map(lambda step: step.action, trajectory.steps))\n",
    "            \n",
    "    if len(obs) == 0:\n",
    "        print(reward_treshold)\n",
    "        print(rewards)\n",
    "    obs = torch.FloatTensor(obs)\n",
    "    action = torch.LongTensor(action)\n",
    "    \n",
    "    return obs, action, reward_treshold, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Part of the Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss 0.70, Mean Reward of the last 16 episodes: 19.62\n",
      "Step 1, Loss 0.70, Mean Reward of the last 16 episodes: 22.12\n",
      "Step 2, Loss 0.69, Mean Reward of the last 16 episodes: 23.31\n",
      "Step 3, Loss 0.69, Mean Reward of the last 16 episodes: 20.69\n",
      "Step 4, Loss 0.69, Mean Reward of the last 16 episodes: 22.31\n",
      "Step 5, Loss 0.69, Mean Reward of the last 16 episodes: 29.38\n",
      "Step 6, Loss 0.69, Mean Reward of the last 16 episodes: 18.06\n",
      "Step 7, Loss 0.69, Mean Reward of the last 16 episodes: 21.12\n",
      "Step 8, Loss 0.68, Mean Reward of the last 16 episodes: 31.00\n",
      "Step 9, Loss 0.68, Mean Reward of the last 16 episodes: 23.06\n",
      "Step 10, Loss 0.68, Mean Reward of the last 16 episodes: 26.88\n",
      "Step 11, Loss 0.68, Mean Reward of the last 16 episodes: 30.25\n",
      "Step 12, Loss 0.68, Mean Reward of the last 16 episodes: 25.62\n",
      "Step 13, Loss 0.68, Mean Reward of the last 16 episodes: 31.94\n",
      "Step 14, Loss 0.67, Mean Reward of the last 16 episodes: 27.88\n",
      "Step 15, Loss 0.67, Mean Reward of the last 16 episodes: 32.62\n",
      "Step 16, Loss 0.67, Mean Reward of the last 16 episodes: 30.50\n",
      "Step 17, Loss 0.66, Mean Reward of the last 16 episodes: 30.38\n",
      "Step 18, Loss 0.68, Mean Reward of the last 16 episodes: 27.50\n",
      "Step 19, Loss 0.67, Mean Reward of the last 16 episodes: 26.19\n",
      "Step 20, Loss 0.68, Mean Reward of the last 16 episodes: 30.12\n",
      "Step 21, Loss 0.66, Mean Reward of the last 16 episodes: 38.94\n",
      "Step 22, Loss 0.66, Mean Reward of the last 16 episodes: 35.31\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(Batch_Selection(env, net, BATCH_SIZE)):\n",
    "    \n",
    "    # Next line is needed for the selection of the top k-percentile episodes.\n",
    "    obs, Best_Act, rewardbt, reward_m = Top_Percentile(batch, PERCENTILE)\n",
    "    Old_Act = net(obs)\n",
    "    loss = criterion(Old_Act, Best_Act)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Next Line is for printing results\n",
    "    print('Step %.d, Loss %.2f, Mean Reward of the last %.d episodes: %.2f'%(idx, loss.item(), BATCH_SIZE, reward_m))\n",
    "    \n",
    "    if reward_m > 199:\n",
    "        print('The environment has been solved')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
