{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import random \n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions import Normal\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Pendulum-vo'\n",
    "batch_size = 32\n",
    "\n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, seed):\n",
    "        random.seed(seed)\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        \n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = int((self.position + 1)%self.capacity)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    # TO DO SEE HOW THIS INTERACTS WITH MY CODE AND MAYBE CHANGE IT\n",
    "    \n",
    "    def save_buffer(self, env_name, suffix = \"\", save_path = None):\n",
    "        if not os.path.exists('checkpoints/'):\n",
    "            os.makedirs('checkpoints/')\n",
    "            \n",
    "        if save_path is None:\n",
    "            save_path = 'checkpoints/sac_buffer_{}_{}'.format(env_name, suffix)\n",
    "        print('Saving buffer to {}'.format(save_path))\n",
    "        \n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dumb(self.buffer, f)\n",
    "            \n",
    "    def load_buffer(self, save_path):\n",
    "        print('Loading buffer from {}'.format(save_path))\n",
    "        \n",
    "        with open(save_path, 'rb') as f:\n",
    "            self.buffer = pickle.load(f)\n",
    "            self.position = len(self.buffer) % self.capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain = 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.apply(weights_init_)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        # First Q Net\n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Second Q Net\n",
    "        self.linear4 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear6 = nn.Linear(hidden_dim, 1)      \n",
    "        \n",
    "        self.apply(weights_init_)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        \n",
    "        x1 = F.relu(self.linear1(x))\n",
    "        x1 = F.relu(self.linear2(x1))\n",
    "        x1 = self.linear3(x1)\n",
    "        \n",
    "        x2 = F.relu(self.linear4(x))\n",
    "        x2 = F.relu(self.linear5(x2))\n",
    "        x2 = self.linear6(x2)\n",
    "        \n",
    "        return x1, x2\n",
    "    \n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space = None):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.mean_linear = nn.Linear(hidden_dim, num_actions)\n",
    "        self.log_std_linear = nn.Linear(hidden_dim, num_actions)\n",
    "        \n",
    "        self.apply(weights_init_)\n",
    "        \n",
    "        if action_space is None:\n",
    "            self.action_scale = torch.tensor(1.)\n",
    "            self.action_bias = torch.tensor(0.)\n",
    "        else:\n",
    "            self.action_scale = torch.FloatTensor((action_space.high-action_space.low)/2.)\n",
    "            self.action_bias = torch.FloatTensor((action_space.high + action_space.low)/2.)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mean = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, min = LOG_SIG_MIN, max = LOG_SIG_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        x_t = normal.rsample()\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # This part is unclear and has to be checked\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + epsilon)\n",
    "        log_prob = log_prob.sum(1, keepdim = True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "\n",
    "        return action, log_prob, mean\n",
    "\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super(GaussianPolicy, self).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova = torch.FloatTensor([[1., 1.],[0.,0.]])\n",
    "prova.sum(1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (linear1): Linear(in_features=10, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (linear3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (linear4): Linear(in_features=10, out_features=128, bias=True)\n",
       "  (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (linear6): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QNetwork(8, 2, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "        \n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.372231605069942e-09"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.prod(torch.FloatTensor(2,)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1, requires_grad = True, device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(object):\n",
    "    def __init__(self, num_inputs, action_space, dictionary):\n",
    "        self.gamma = dictionary['gamma']\n",
    "        self.tau = dictionary['tau']\n",
    "        self.alpha = dictionary['alpha']\n",
    "        self.target_update_interval = dictionary['target_update_interval']\n",
    "        \n",
    "        self.policy_type = dictionary['policy']\n",
    "        self.automatic_entropy_tuning = dictionary['automatic_entropy_tuning']\n",
    "        \n",
    "        self.device = torch.device('cuda' if dictionary['cuda'] else 'cpu')\n",
    "        \n",
    "        self.lr1= dictionary['lr1']\n",
    "        self.lr2 = dictionary['lr2']\n",
    "        self.lr3 = dictionary['lr3']\n",
    "        #print('num inputs {} action space {} dictionary hidden'.format(num_inputs, action_space.shape[0]))\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], dictionary['hidden_size']).to(device = self.device)\n",
    "        self.critic_tg = QNetwork(num_inputs, action_space.shape[0], dictionary['hidden_size']).to(device = self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), self.lr1)\n",
    "        \n",
    "        hard_update(self.critic_tg, self.critic)\n",
    "        #print(self.device)\n",
    "        if self.policy_type == 'Gaussian':\n",
    "            #print('ciao')\n",
    "            if self.automatic_entropy_tuning is True:\n",
    "                self.target_entropy = -torch.prod(torch.Tensor(action_space.shape).to(self.device)).item()\n",
    "                self.log_alpha = torch.zeros(1, requires_grad = True, device = self.device)\n",
    "                self.alpha_optim = Adam([self.log_alpha], lr = self.lr2)\n",
    "                \n",
    "            self.policy = GaussianPolicy(num_inputs, action_space.shape[0], dictionary['hidden_size'], action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), self.lr3)\n",
    "                              \n",
    "        # TO DO Add deterministic \n",
    "                              \n",
    "    def select_action(self, state, evaluate = False):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if evaluate is False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size = batch_size)\n",
    "        \n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_state_action, next_state_log_pi, _ = self.policy.sample(next_state_batch)\n",
    "            qf1_next_target, qf2_next_target = self.critic_tg(next_state_batch, next_state_action)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
    "            next_q_value = reward_batch + mask_batch * self.gamma * (min_qf_next_target)\n",
    "            \n",
    "        qf1, qf2 = self.critic(state_batch, action_batch)\n",
    "        qf1_loss = F.mse_loss(qf1, next_q_value)\n",
    "        qf2_loss = F.mse_loss(qf2, next_q_value)\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "        \n",
    "        self.critic_optim.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        pi, log_pi, _ = self.policy.sample(state_batch)\n",
    "        qf1_pi, qf2_pi = self.critic(state_batch, pi)\n",
    "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "        \n",
    "        policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean()\n",
    "        \n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "        \n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "            \n",
    "            self.alpha_optim.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optim.step()\n",
    "            \n",
    "            self.alpha = self.log_alpha.exp()\n",
    "            alpha_tlogs = self.alpha.clone()\n",
    "            \n",
    "        else:\n",
    "            alpha_loss = torch.tensor(0.).to(self.device)\n",
    "            alpha_tlogs = torch.tensor(self.alpha)\n",
    "            \n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_tg, self.critic, self.tau)\n",
    "\n",
    "        return qf1_loss.item(), qf2_loss.item(), policy_loss.item(), alpha_loss.item(), alpha_tlogs.item()\n",
    "    \n",
    "    def save_checkpoint(self, env_name, suffix=\"\", ckpt_path=None):\n",
    "        if not os.path.exists('checkpoints/'):\n",
    "            os.makedirs('checkpoints/')\n",
    "        if ckpt_path is None:\n",
    "            ckpt_path = \"checkpoints/sac_checkpoint_{}_{}\".format(env_name, suffix)\n",
    "        print('Saving models to {}'.format(ckpt_path))\n",
    "        torch.save({'policy_state_dict': self.policy.state_dict(),\n",
    "                    'critic_state_dict': self.critic.state_dict(),\n",
    "                    'critic_target_state_dict': self.critic_tg.state_dict(),\n",
    "                    'critic_optimizer_state_dict': self.critic_optim.state_dict(),\n",
    "                    'policy_optimizer_state_dict': self.policy_optim.state_dict()}, ckpt_path)\n",
    "\n",
    "    # Load model parameters\n",
    "    def load_checkpoint(self, ckpt_path, evaluate=False):\n",
    "        print('Loading models from {}'.format(ckpt_path))\n",
    "        if ckpt_path is not None:\n",
    "            checkpoint = torch.load(ckpt_path)\n",
    "            self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "            self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "            self.critic_tg.load_state_dict(checkpoint['critic_target_state_dict'])\n",
    "            self.critic_optim.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "            self.policy_optim.load_state_dict(checkpoint['policy_optimizer_state_dict'])\n",
    "\n",
    "            if evaluate:\n",
    "                self.policy.eval()\n",
    "                self.critic.eval()\n",
    "                self.critic_tg.eval()\n",
    "            else:\n",
    "                self.policy.train()\n",
    "                self.critic.train()\n",
    "                self.critic_tg.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Instatiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try to install mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(8,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "env.seed(5)\n",
    "env.action_space.seed(5)\n",
    "torch.manual_seed(5)\n",
    "np.random.seed(5)\n",
    "print(env.action_space.shape)\n",
    "print(env.observation_space.shape)\n",
    "env._max_episode_steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent and Memory Instatiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary ={'gamma': 0.98,\n",
    "             'tau': 0.9,\n",
    "             'alpha': 0.5,\n",
    "             'target_update_interval': 20,\n",
    "             'policy': 'Gaussian',\n",
    "             'cuda': True,\n",
    "             'lr1': 1e-4,\n",
    "             'lr2': 1e-4,\n",
    "             'lr3': 1e-4,\n",
    "             'hidden_size': 128,\n",
    "             'automatic_entropy_tuning': True,\n",
    "            }\n",
    "\n",
    "capacity_size = 1e6\n",
    "seed = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SAC(env.observation_space.shape[0], env.action_space, dictionary)\n",
    "memory = ReplayMemory(capacity_size, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Instatiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "automatic_entropy_tuning = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/{}_SAC_{}_{}_{}'.format(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), 'Pendulum-v0','Gaussian', 'autotune' if automatic_entropy_tuning else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "initial_step = 500\n",
    "max_num_steps = np.inf\n",
    "evaluation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, total numsteps: 97, episode steps: 97, reward: -269.27\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -150.44\n",
      "----------------------------------------\n",
      "Episode: 1, total numsteps: 197, episode steps: 100, reward: -300.62\n",
      "Episode: 2, total numsteps: 317, episode steps: 120, reward: -431.93\n",
      "Episode: 3, total numsteps: 455, episode steps: 138, reward: -305.36\n",
      "Episode: 4, total numsteps: 578, episode steps: 123, reward: -136.1\n",
      "Episode: 5, total numsteps: 648, episode steps: 70, reward: -72.01\n",
      "Episode: 6, total numsteps: 742, episode steps: 94, reward: -262.11\n",
      "Episode: 7, total numsteps: 865, episode steps: 123, reward: -134.76\n",
      "Episode: 8, total numsteps: 978, episode steps: 113, reward: -27.82\n",
      "Episode: 9, total numsteps: 1100, episode steps: 122, reward: -284.35\n",
      "Episode: 10, total numsteps: 1255, episode steps: 155, reward: -87.95\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -130.09\n",
      "----------------------------------------\n",
      "Episode: 11, total numsteps: 1346, episode steps: 91, reward: -298.67\n",
      "Episode: 12, total numsteps: 1430, episode steps: 84, reward: -90.24\n",
      "Episode: 13, total numsteps: 1557, episode steps: 127, reward: -189.35\n",
      "Episode: 14, total numsteps: 1635, episode steps: 78, reward: -196.48\n",
      "Episode: 15, total numsteps: 1762, episode steps: 127, reward: -405.75\n",
      "Episode: 16, total numsteps: 1858, episode steps: 96, reward: -259.47\n",
      "Episode: 17, total numsteps: 2019, episode steps: 161, reward: -76.75\n",
      "Episode: 18, total numsteps: 2167, episode steps: 148, reward: -59.1\n",
      "Episode: 19, total numsteps: 2302, episode steps: 135, reward: -147.97\n",
      "Episode: 20, total numsteps: 2452, episode steps: 150, reward: -249.14\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -240.08\n",
      "----------------------------------------\n",
      "Episode: 21, total numsteps: 2645, episode steps: 193, reward: -279.28\n",
      "Episode: 22, total numsteps: 2809, episode steps: 164, reward: -332.26\n",
      "Episode: 23, total numsteps: 2989, episode steps: 180, reward: -315.05\n",
      "Episode: 24, total numsteps: 3146, episode steps: 157, reward: -261.53\n",
      "Episode: 25, total numsteps: 3369, episode steps: 223, reward: -263.05\n",
      "Episode: 26, total numsteps: 3587, episode steps: 218, reward: -251.89\n",
      "Episode: 27, total numsteps: 3775, episode steps: 188, reward: -195.13\n",
      "Episode: 28, total numsteps: 4093, episode steps: 318, reward: -299.2\n",
      "Episode: 29, total numsteps: 4430, episode steps: 337, reward: -37.75\n",
      "Episode: 30, total numsteps: 4700, episode steps: 270, reward: -214.11\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -403.53\n",
      "----------------------------------------\n",
      "Episode: 31, total numsteps: 4986, episode steps: 286, reward: -319.07\n",
      "Episode: 32, total numsteps: 5456, episode steps: 470, reward: -255.84\n",
      "Episode: 33, total numsteps: 5879, episode steps: 423, reward: -427.27\n",
      "Episode: 34, total numsteps: 6248, episode steps: 369, reward: -295.67\n",
      "Episode: 35, total numsteps: 6570, episode steps: 322, reward: -270.09\n",
      "Episode: 36, total numsteps: 6961, episode steps: 391, reward: -292.47\n",
      "Episode: 37, total numsteps: 7480, episode steps: 519, reward: -139.52\n",
      "Episode: 38, total numsteps: 8117, episode steps: 637, reward: -129.6\n",
      "Episode: 39, total numsteps: 9117, episode steps: 1000, reward: -118.56\n",
      "Episode: 40, total numsteps: 9999, episode steps: 882, reward: -354.86\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -201.08\n",
      "----------------------------------------\n",
      "Episode: 41, total numsteps: 10832, episode steps: 833, reward: -186.78\n",
      "Episode: 42, total numsteps: 11289, episode steps: 457, reward: -130.74\n",
      "Episode: 43, total numsteps: 11480, episode steps: 191, reward: -68.57\n",
      "Episode: 44, total numsteps: 11858, episode steps: 378, reward: -55.52\n",
      "Episode: 45, total numsteps: 12823, episode steps: 965, reward: -228.57\n",
      "Episode: 46, total numsteps: 13313, episode steps: 490, reward: -167.93\n",
      "Episode: 47, total numsteps: 13474, episode steps: 161, reward: -19.48\n",
      "Episode: 48, total numsteps: 14268, episode steps: 794, reward: -214.9\n",
      "Episode: 49, total numsteps: 14483, episode steps: 215, reward: -70.63\n",
      "Episode: 50, total numsteps: 15483, episode steps: 1000, reward: -76.42\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -114.4\n",
      "----------------------------------------\n",
      "Episode: 51, total numsteps: 15995, episode steps: 512, reward: -120.29\n",
      "Episode: 52, total numsteps: 16995, episode steps: 1000, reward: -97.79\n",
      "Episode: 53, total numsteps: 17995, episode steps: 1000, reward: -92.29\n",
      "Episode: 54, total numsteps: 18995, episode steps: 1000, reward: -118.26\n",
      "Episode: 55, total numsteps: 19995, episode steps: 1000, reward: -16.54\n",
      "Episode: 56, total numsteps: 20305, episode steps: 310, reward: -3.09\n",
      "Episode: 57, total numsteps: 20440, episode steps: 135, reward: -85.3\n",
      "Episode: 58, total numsteps: 21440, episode steps: 1000, reward: -35.05\n",
      "Episode: 59, total numsteps: 22440, episode steps: 1000, reward: -33.88\n",
      "Episode: 60, total numsteps: 23440, episode steps: 1000, reward: -47.56\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -60.04\n",
      "----------------------------------------\n",
      "Episode: 61, total numsteps: 24440, episode steps: 1000, reward: -23.1\n",
      "Episode: 62, total numsteps: 25440, episode steps: 1000, reward: -17.14\n",
      "Episode: 63, total numsteps: 26440, episode steps: 1000, reward: -20.97\n",
      "Episode: 64, total numsteps: 27440, episode steps: 1000, reward: -35.19\n",
      "Episode: 65, total numsteps: 28440, episode steps: 1000, reward: -44.54\n",
      "Episode: 66, total numsteps: 29440, episode steps: 1000, reward: 3.78\n",
      "Episode: 67, total numsteps: 30440, episode steps: 1000, reward: -28.92\n",
      "Episode: 68, total numsteps: 31440, episode steps: 1000, reward: -19.77\n",
      "Episode: 69, total numsteps: 32440, episode steps: 1000, reward: -73.1\n",
      "Episode: 70, total numsteps: 33440, episode steps: 1000, reward: -43.99\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -93.17\n",
      "----------------------------------------\n",
      "Episode: 71, total numsteps: 34440, episode steps: 1000, reward: -85.11\n",
      "Episode: 72, total numsteps: 35440, episode steps: 1000, reward: -82.69\n",
      "Episode: 73, total numsteps: 36440, episode steps: 1000, reward: -37.55\n",
      "Episode: 74, total numsteps: 37440, episode steps: 1000, reward: -81.1\n",
      "Episode: 75, total numsteps: 38440, episode steps: 1000, reward: -94.82\n",
      "Episode: 76, total numsteps: 39440, episode steps: 1000, reward: -71.05\n",
      "Episode: 77, total numsteps: 40440, episode steps: 1000, reward: -108.34\n",
      "Episode: 78, total numsteps: 41440, episode steps: 1000, reward: -99.13\n",
      "Episode: 79, total numsteps: 42440, episode steps: 1000, reward: -118.55\n",
      "Episode: 80, total numsteps: 43440, episode steps: 1000, reward: -105.89\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -83.88\n",
      "----------------------------------------\n",
      "Episode: 81, total numsteps: 44375, episode steps: 935, reward: -240.47\n",
      "Episode: 82, total numsteps: 45375, episode steps: 1000, reward: -68.64\n",
      "Episode: 83, total numsteps: 46375, episode steps: 1000, reward: -70.72\n",
      "Episode: 84, total numsteps: 47375, episode steps: 1000, reward: -68.43\n",
      "Episode: 85, total numsteps: 48375, episode steps: 1000, reward: -27.01\n",
      "Episode: 86, total numsteps: 49375, episode steps: 1000, reward: -62.8\n",
      "Episode: 87, total numsteps: 50375, episode steps: 1000, reward: -107.75\n",
      "Episode: 88, total numsteps: 51375, episode steps: 1000, reward: -92.37\n",
      "Episode: 89, total numsteps: 52375, episode steps: 1000, reward: -58.53\n",
      "Episode: 90, total numsteps: 53375, episode steps: 1000, reward: -88.31\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -87.46\n",
      "----------------------------------------\n",
      "Episode: 91, total numsteps: 54375, episode steps: 1000, reward: -106.69\n",
      "Episode: 92, total numsteps: 55375, episode steps: 1000, reward: -76.14\n",
      "Episode: 93, total numsteps: 56051, episode steps: 676, reward: -185.69\n",
      "Episode: 94, total numsteps: 57051, episode steps: 1000, reward: -94.15\n",
      "Episode: 95, total numsteps: 57858, episode steps: 807, reward: -228.44\n",
      "Episode: 96, total numsteps: 58513, episode steps: 655, reward: -182.11\n",
      "Episode: 97, total numsteps: 58826, episode steps: 313, reward: -99.45\n",
      "Episode: 98, total numsteps: 59364, episode steps: 538, reward: -127.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 99, total numsteps: 60364, episode steps: 1000, reward: -23.69\n",
      "Episode: 100, total numsteps: 61364, episode steps: 1000, reward: -57.23\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -58.61\n",
      "----------------------------------------\n",
      "Episode: 101, total numsteps: 62364, episode steps: 1000, reward: -21.22\n",
      "Episode: 102, total numsteps: 63364, episode steps: 1000, reward: -76.07\n",
      "Episode: 103, total numsteps: 64364, episode steps: 1000, reward: 6.75\n",
      "Episode: 104, total numsteps: 65364, episode steps: 1000, reward: -6.35\n",
      "Episode: 105, total numsteps: 66364, episode steps: 1000, reward: -44.02\n",
      "Episode: 106, total numsteps: 67364, episode steps: 1000, reward: -89.16\n",
      "Episode: 107, total numsteps: 68364, episode steps: 1000, reward: 22.42\n",
      "Episode: 108, total numsteps: 69364, episode steps: 1000, reward: 36.8\n",
      "Episode: 109, total numsteps: 70364, episode steps: 1000, reward: -19.88\n",
      "Episode: 110, total numsteps: 71364, episode steps: 1000, reward: -96.11\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -48.38\n",
      "----------------------------------------\n",
      "Episode: 111, total numsteps: 72364, episode steps: 1000, reward: -12.55\n",
      "Episode: 112, total numsteps: 73364, episode steps: 1000, reward: -36.72\n",
      "Episode: 113, total numsteps: 74364, episode steps: 1000, reward: -1.91\n",
      "Episode: 114, total numsteps: 75364, episode steps: 1000, reward: 17.7\n",
      "Episode: 115, total numsteps: 76008, episode steps: 644, reward: -143.98\n",
      "Episode: 116, total numsteps: 77008, episode steps: 1000, reward: -32.98\n",
      "Episode: 117, total numsteps: 78008, episode steps: 1000, reward: -34.59\n",
      "Episode: 118, total numsteps: 79008, episode steps: 1000, reward: -69.87\n",
      "Episode: 119, total numsteps: 80008, episode steps: 1000, reward: -73.28\n",
      "Episode: 120, total numsteps: 81008, episode steps: 1000, reward: -59.06\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -39.52\n",
      "----------------------------------------\n",
      "Episode: 121, total numsteps: 82008, episode steps: 1000, reward: -30.18\n",
      "Episode: 122, total numsteps: 83008, episode steps: 1000, reward: -51.34\n",
      "Episode: 123, total numsteps: 84008, episode steps: 1000, reward: -45.75\n",
      "Episode: 124, total numsteps: 85008, episode steps: 1000, reward: -11.25\n",
      "Episode: 125, total numsteps: 86008, episode steps: 1000, reward: -92.26\n",
      "Episode: 126, total numsteps: 87008, episode steps: 1000, reward: -39.34\n",
      "Episode: 127, total numsteps: 88008, episode steps: 1000, reward: -65.58\n",
      "Episode: 128, total numsteps: 89008, episode steps: 1000, reward: -147.27\n",
      "Episode: 129, total numsteps: 90008, episode steps: 1000, reward: -20.67\n",
      "Episode: 130, total numsteps: 91008, episode steps: 1000, reward: -51.25\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -52.77\n",
      "----------------------------------------\n",
      "Episode: 131, total numsteps: 92008, episode steps: 1000, reward: -45.71\n",
      "Episode: 132, total numsteps: 93008, episode steps: 1000, reward: -30.27\n",
      "Episode: 133, total numsteps: 94008, episode steps: 1000, reward: -80.85\n",
      "Episode: 134, total numsteps: 95008, episode steps: 1000, reward: -80.45\n",
      "Episode: 135, total numsteps: 96008, episode steps: 1000, reward: -66.98\n",
      "Episode: 136, total numsteps: 97008, episode steps: 1000, reward: -59.49\n",
      "Episode: 137, total numsteps: 98008, episode steps: 1000, reward: -83.24\n",
      "Episode: 138, total numsteps: 99008, episode steps: 1000, reward: 7.57\n",
      "Episode: 139, total numsteps: 100008, episode steps: 1000, reward: 1.74\n",
      "Episode: 140, total numsteps: 101008, episode steps: 1000, reward: -42.56\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -80.4\n",
      "----------------------------------------\n",
      "Episode: 141, total numsteps: 102008, episode steps: 1000, reward: -8.13\n",
      "Episode: 142, total numsteps: 103008, episode steps: 1000, reward: -50.98\n",
      "Episode: 143, total numsteps: 104008, episode steps: 1000, reward: -66.23\n",
      "Episode: 144, total numsteps: 105008, episode steps: 1000, reward: -66.96\n",
      "Episode: 145, total numsteps: 106008, episode steps: 1000, reward: 4.86\n",
      "Episode: 146, total numsteps: 107008, episode steps: 1000, reward: -75.46\n",
      "Episode: 147, total numsteps: 108008, episode steps: 1000, reward: -95.15\n",
      "Episode: 148, total numsteps: 108658, episode steps: 650, reward: -168.44\n",
      "Episode: 149, total numsteps: 109658, episode steps: 1000, reward: -23.41\n",
      "Episode: 150, total numsteps: 110658, episode steps: 1000, reward: -96.77\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -56.23\n",
      "----------------------------------------\n",
      "Episode: 151, total numsteps: 111658, episode steps: 1000, reward: -15.07\n",
      "Episode: 152, total numsteps: 112590, episode steps: 932, reward: -138.79\n",
      "Episode: 153, total numsteps: 113590, episode steps: 1000, reward: -43.5\n",
      "Episode: 154, total numsteps: 114590, episode steps: 1000, reward: -36.8\n",
      "Episode: 155, total numsteps: 115590, episode steps: 1000, reward: -36.97\n",
      "Episode: 156, total numsteps: 116590, episode steps: 1000, reward: -59.73\n",
      "Episode: 157, total numsteps: 116669, episode steps: 79, reward: 20.59\n",
      "Episode: 158, total numsteps: 117669, episode steps: 1000, reward: -21.72\n",
      "Episode: 159, total numsteps: 118669, episode steps: 1000, reward: -28.85\n",
      "Episode: 160, total numsteps: 119669, episode steps: 1000, reward: -105.18\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -78.02\n",
      "----------------------------------------\n",
      "Episode: 161, total numsteps: 120669, episode steps: 1000, reward: -55.16\n",
      "Episode: 162, total numsteps: 121669, episode steps: 1000, reward: -78.32\n",
      "Episode: 163, total numsteps: 122669, episode steps: 1000, reward: -37.62\n",
      "Episode: 164, total numsteps: 123669, episode steps: 1000, reward: -101.59\n",
      "Episode: 165, total numsteps: 124669, episode steps: 1000, reward: -66.12\n",
      "Episode: 166, total numsteps: 125669, episode steps: 1000, reward: -42.07\n",
      "Episode: 167, total numsteps: 126274, episode steps: 605, reward: -125.1\n",
      "Episode: 168, total numsteps: 127274, episode steps: 1000, reward: -57.8\n",
      "Episode: 169, total numsteps: 128274, episode steps: 1000, reward: -101.92\n",
      "Episode: 170, total numsteps: 129274, episode steps: 1000, reward: -60.69\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -97.68\n",
      "----------------------------------------\n",
      "Episode: 171, total numsteps: 130274, episode steps: 1000, reward: -26.56\n",
      "Episode: 172, total numsteps: 131274, episode steps: 1000, reward: -63.18\n",
      "Episode: 173, total numsteps: 132274, episode steps: 1000, reward: -45.26\n",
      "Episode: 174, total numsteps: 133274, episode steps: 1000, reward: -56.75\n",
      "Episode: 175, total numsteps: 134274, episode steps: 1000, reward: 0.42\n",
      "Episode: 176, total numsteps: 135274, episode steps: 1000, reward: -20.59\n",
      "Episode: 177, total numsteps: 136274, episode steps: 1000, reward: -16.7\n",
      "Episode: 178, total numsteps: 137274, episode steps: 1000, reward: -77.56\n",
      "Episode: 179, total numsteps: 138274, episode steps: 1000, reward: -98.83\n",
      "Episode: 180, total numsteps: 139274, episode steps: 1000, reward: 10.48\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -35.32\n",
      "----------------------------------------\n",
      "Episode: 181, total numsteps: 140274, episode steps: 1000, reward: -48.96\n",
      "Episode: 182, total numsteps: 141274, episode steps: 1000, reward: -42.11\n",
      "Episode: 183, total numsteps: 142274, episode steps: 1000, reward: -50.57\n",
      "Episode: 184, total numsteps: 143274, episode steps: 1000, reward: -16.09\n",
      "Episode: 185, total numsteps: 144274, episode steps: 1000, reward: -64.77\n",
      "Episode: 186, total numsteps: 145274, episode steps: 1000, reward: -66.14\n",
      "Episode: 187, total numsteps: 146274, episode steps: 1000, reward: -2.63\n",
      "Episode: 188, total numsteps: 147274, episode steps: 1000, reward: -6.01\n",
      "Episode: 189, total numsteps: 148274, episode steps: 1000, reward: -49.67\n",
      "Episode: 190, total numsteps: 149274, episode steps: 1000, reward: -24.94\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -30.08\n",
      "----------------------------------------\n",
      "Episode: 191, total numsteps: 150274, episode steps: 1000, reward: 8.62\n",
      "Episode: 192, total numsteps: 151274, episode steps: 1000, reward: -36.64\n",
      "Episode: 193, total numsteps: 152274, episode steps: 1000, reward: 35.1\n",
      "Episode: 194, total numsteps: 153274, episode steps: 1000, reward: 4.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 195, total numsteps: 154274, episode steps: 1000, reward: -65.9\n",
      "Episode: 196, total numsteps: 155274, episode steps: 1000, reward: -51.27\n",
      "Episode: 197, total numsteps: 156274, episode steps: 1000, reward: -15.66\n",
      "Episode: 198, total numsteps: 157274, episode steps: 1000, reward: -16.41\n",
      "Episode: 199, total numsteps: 158274, episode steps: 1000, reward: 11.97\n",
      "Episode: 200, total numsteps: 159274, episode steps: 1000, reward: -4.06\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -9.41\n",
      "----------------------------------------\n",
      "Episode: 201, total numsteps: 160274, episode steps: 1000, reward: -7.85\n",
      "Episode: 202, total numsteps: 161274, episode steps: 1000, reward: -35.32\n",
      "Episode: 203, total numsteps: 162274, episode steps: 1000, reward: 6.36\n",
      "Episode: 204, total numsteps: 163274, episode steps: 1000, reward: -50.04\n",
      "Episode: 205, total numsteps: 164274, episode steps: 1000, reward: 8.82\n",
      "Episode: 206, total numsteps: 165274, episode steps: 1000, reward: -30.26\n",
      "Episode: 207, total numsteps: 166274, episode steps: 1000, reward: -32.5\n",
      "Episode: 208, total numsteps: 167274, episode steps: 1000, reward: -49.3\n",
      "Episode: 209, total numsteps: 168274, episode steps: 1000, reward: -24.17\n",
      "Episode: 210, total numsteps: 169274, episode steps: 1000, reward: -16.81\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -20.66\n",
      "----------------------------------------\n",
      "Episode: 211, total numsteps: 170274, episode steps: 1000, reward: -15.49\n",
      "Episode: 212, total numsteps: 171274, episode steps: 1000, reward: -12.78\n",
      "Episode: 213, total numsteps: 172274, episode steps: 1000, reward: -16.05\n",
      "Episode: 214, total numsteps: 173274, episode steps: 1000, reward: -47.15\n",
      "Episode: 215, total numsteps: 174022, episode steps: 748, reward: -86.23\n",
      "Episode: 216, total numsteps: 175022, episode steps: 1000, reward: 11.91\n",
      "Episode: 217, total numsteps: 176022, episode steps: 1000, reward: -39.18\n",
      "Episode: 218, total numsteps: 177022, episode steps: 1000, reward: 0.53\n",
      "Episode: 219, total numsteps: 178022, episode steps: 1000, reward: -36.49\n",
      "Episode: 220, total numsteps: 179022, episode steps: 1000, reward: -37.66\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -27.51\n",
      "----------------------------------------\n",
      "Episode: 221, total numsteps: 180022, episode steps: 1000, reward: -26.23\n",
      "Episode: 222, total numsteps: 181022, episode steps: 1000, reward: -67.48\n",
      "Episode: 223, total numsteps: 182022, episode steps: 1000, reward: -22.72\n",
      "Episode: 224, total numsteps: 183022, episode steps: 1000, reward: -57.99\n",
      "Episode: 225, total numsteps: 184022, episode steps: 1000, reward: -0.56\n",
      "Episode: 226, total numsteps: 185022, episode steps: 1000, reward: -34.38\n",
      "Episode: 227, total numsteps: 186022, episode steps: 1000, reward: -52.27\n",
      "Episode: 228, total numsteps: 187022, episode steps: 1000, reward: -86.31\n",
      "Episode: 229, total numsteps: 188022, episode steps: 1000, reward: -27.63\n",
      "Episode: 230, total numsteps: 189022, episode steps: 1000, reward: -8.55\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -63.96\n",
      "----------------------------------------\n",
      "Episode: 231, total numsteps: 190022, episode steps: 1000, reward: -3.08\n",
      "Episode: 232, total numsteps: 191022, episode steps: 1000, reward: -7.18\n",
      "Episode: 233, total numsteps: 192022, episode steps: 1000, reward: -21.52\n",
      "Episode: 234, total numsteps: 193002, episode steps: 980, reward: -189.22\n",
      "Episode: 235, total numsteps: 194002, episode steps: 1000, reward: -103.1\n",
      "Episode: 236, total numsteps: 195002, episode steps: 1000, reward: -21.01\n",
      "Episode: 237, total numsteps: 196002, episode steps: 1000, reward: -38.08\n",
      "Episode: 238, total numsteps: 197002, episode steps: 1000, reward: -44.11\n",
      "Episode: 239, total numsteps: 198002, episode steps: 1000, reward: -42.57\n",
      "Episode: 240, total numsteps: 199002, episode steps: 1000, reward: -61.06\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -30.54\n",
      "----------------------------------------\n",
      "Episode: 241, total numsteps: 200002, episode steps: 1000, reward: -26.43\n",
      "Episode: 242, total numsteps: 201002, episode steps: 1000, reward: -68.98\n",
      "Episode: 243, total numsteps: 202002, episode steps: 1000, reward: -5.68\n",
      "Episode: 244, total numsteps: 203002, episode steps: 1000, reward: -73.55\n",
      "Episode: 245, total numsteps: 204002, episode steps: 1000, reward: -34.52\n",
      "Episode: 246, total numsteps: 205002, episode steps: 1000, reward: -43.65\n",
      "Episode: 247, total numsteps: 206002, episode steps: 1000, reward: -43.42\n",
      "Episode: 248, total numsteps: 207002, episode steps: 1000, reward: -12.92\n",
      "Episode: 249, total numsteps: 208002, episode steps: 1000, reward: -37.55\n",
      "Episode: 250, total numsteps: 209002, episode steps: 1000, reward: -36.88\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -50.73\n",
      "----------------------------------------\n",
      "Episode: 251, total numsteps: 210002, episode steps: 1000, reward: -60.09\n",
      "Episode: 252, total numsteps: 211002, episode steps: 1000, reward: -35.29\n",
      "Episode: 253, total numsteps: 212002, episode steps: 1000, reward: -45.79\n",
      "Episode: 254, total numsteps: 213002, episode steps: 1000, reward: -4.13\n",
      "Episode: 255, total numsteps: 214002, episode steps: 1000, reward: -5.54\n",
      "Episode: 256, total numsteps: 215002, episode steps: 1000, reward: -17.15\n",
      "Episode: 257, total numsteps: 216002, episode steps: 1000, reward: -32.21\n",
      "Episode: 258, total numsteps: 217002, episode steps: 1000, reward: -41.28\n",
      "Episode: 259, total numsteps: 218002, episode steps: 1000, reward: -8.0\n",
      "Episode: 260, total numsteps: 219002, episode steps: 1000, reward: 0.66\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -44.03\n",
      "----------------------------------------\n",
      "Episode: 261, total numsteps: 220002, episode steps: 1000, reward: -43.04\n",
      "Episode: 262, total numsteps: 221002, episode steps: 1000, reward: -41.2\n",
      "Episode: 263, total numsteps: 222002, episode steps: 1000, reward: -55.74\n",
      "Episode: 264, total numsteps: 223002, episode steps: 1000, reward: -32.57\n",
      "Episode: 265, total numsteps: 224002, episode steps: 1000, reward: -23.41\n",
      "Episode: 266, total numsteps: 225002, episode steps: 1000, reward: -18.22\n",
      "Episode: 267, total numsteps: 226002, episode steps: 1000, reward: -9.78\n",
      "Episode: 268, total numsteps: 227002, episode steps: 1000, reward: -40.96\n",
      "Episode: 269, total numsteps: 228002, episode steps: 1000, reward: -19.54\n",
      "Episode: 270, total numsteps: 229002, episode steps: 1000, reward: -34.47\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -38.75\n",
      "----------------------------------------\n",
      "Episode: 271, total numsteps: 230002, episode steps: 1000, reward: -11.38\n",
      "Episode: 272, total numsteps: 230693, episode steps: 691, reward: -74.56\n",
      "Episode: 273, total numsteps: 231693, episode steps: 1000, reward: -53.65\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d1af42b1fea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-57e2642d8479>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state, evaluate)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9c12f9bc277b>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_t\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_scale\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;31m# This part is unclear and has to be checked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlog_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_scale\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_numsteps = 0\n",
    "updates = 0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        if initial_step > total_numsteps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.select_action(state)\n",
    "        \n",
    "        if len(memory) > batch_size:\n",
    "            critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, batch_size, updates)\n",
    "            \n",
    "            writer.add_scalar('loss_critic_1', critic_1_loss, updates)\n",
    "            writer.add_scalar('loss_critic_2', critic_2_loss, updates)\n",
    "            writer.add_scalar('loss_policy', policy_loss, updates)\n",
    "            writer.add_scalar('loss_entropy_loss', ent_loss, updates)\n",
    "            writer.add_scalar('entropy_temp_alpha', alpha, updates)\n",
    "            updates += 1\n",
    "            \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_steps += 1\n",
    "        total_numsteps += 1\n",
    "        episode_reward += reward\n",
    "        \n",
    "        mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
    "        \n",
    "        memory.push(state, action, reward, next_state, mask)\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    if total_numsteps > max_num_steps:\n",
    "        break\n",
    "        \n",
    "    writer.add_scalar('reward/train', episode_reward, i_episode)\n",
    "    print(\"Episode: {}, total numsteps: {}, episode steps: {}, reward: {}\".format(i_episode, total_numsteps, episode_steps, round(episode_reward, 2)))\n",
    "\n",
    "    if i_episode % 10 == 0 and evaluation is True:\n",
    "        avg_reward = 0.\n",
    "        episodes = 10\n",
    "        for _  in range(episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.select_action(state, evaluate=True)\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "            avg_reward += episode_reward\n",
    "        avg_reward /= episodes\n",
    "\n",
    "\n",
    "        writer.add_scalar('avg_reward/test', avg_reward, i_episode)\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Test Episodes: {}, Avg. Reward: {}\".format(episodes, round(avg_reward, 2)))\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
